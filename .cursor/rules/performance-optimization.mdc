---
description: Performance optimization patterns and monitoring for durable execution
---

# Performance Optimization

## Executor configuration

```ts
const executor = new DurableExecutor(storage, {
  maxConcurrentTaskExecutions: 100,           // Parallelism cap
  maxTaskExecutionsPerBatch: 25,              // Batch size per storage scan
  backgroundProcessIntraBatchSleepMs: 250,    // Backoff between batches
  expireLeewayMs: 150_000,                    // Expiry recovery window
  storageMaxRetryAttempts: 3,                 // Storage retry wrapper
})
```

Tuning tips:

- Increase `maxConcurrentTaskExecutions` gradually and observe DB/CPU saturation
- Keep `maxTaskExecutionsPerBatch` small enough to minimize long transactions
- Let the adaptive sleep handle idle periods; avoid busy loops

## Storage performance

### Database indexes

Align with storage access patterns:

```sql
-- Identity
CREATE UNIQUE INDEX IF NOT EXISTS ux_task_executions_execution_id
  ON task_executions (execution_id);

-- Ready â†’ running pickup
CREATE INDEX IF NOT EXISTS idx_status_start_at
  ON task_executions (status, start_at);

-- Children-finished processing
CREATE INDEX IF NOT EXISTS idx_status_children_count_updated_at
  ON task_executions (status, on_children_finished_processing_status, active_children_count, updated_at);

-- Close pipeline
CREATE INDEX IF NOT EXISTS idx_close_status_updated_at
  ON task_executions (close_status, updated_at);

-- Expiration recovery
CREATE INDEX IF NOT EXISTS idx_expires_at
  ON task_executions (expires_at);
CREATE INDEX IF NOT EXISTS idx_on_children_finished_processing_expires_at
  ON task_executions (on_children_finished_processing_expires_at);
CREATE INDEX IF NOT EXISTS idx_close_expires_at
  ON task_executions (close_expires_at);

-- Cancellation
CREATE INDEX IF NOT EXISTS idx_executor_id_npc_updated_at
  ON task_executions (executor_id, needs_promise_cancellation, updated_at);

-- Parent relations
CREATE INDEX IF NOT EXISTS idx_parent_execution_id_is_finished
  ON task_executions (parent_execution_id, is_finished);
CREATE INDEX IF NOT EXISTS idx_is_finished_close_status_updated_at
  ON task_executions (is_finished, close_status, updated_at);
```

### Connection pooling

```ts
// Example (PostgreSQL): adjust to your client
const sql = postgres(connectionString, {
  max: 20,
  idle_timeout: 20,
  max_lifetime: 60 * 30,
})
```

## Task design

### Right-sized tasks

```ts
// Good: Manageable chunks
const processFileChunkTask = executor.task({
  id: 'processChunk',
  timeoutMs: 60_000,
  run: (ctx, input: { filePath: string; startByte: number; endByte: number }) => {
    // Process manageable chunk - easy to retry/parallelize
  },
})

// Bad: Too coarse-grained
const processEntireFileTask = executor.task({
  id: 'processFile',
  timeoutMs: 3_600_000,
  run: async () => {
    // Long blocking work - hard to resume
  },
})
```

### Efficient retry strategy

```ts
const optimizedTask = executor.task({
  id: 'apiCall',
  retryOptions: {
    maxAttempts: 3,
    baseDelayMs: 1_000,
    delayMultiplier: 2,   // Exponential backoff
    maxDelayMs: 30_000,
  },
  timeoutMs: 30_000,
  run: async (ctx, input) => {
    if (input.invalid) {
      throw DurableExecutionError.nonRetryable('Invalid input')
    }
    return await externalApi.call(input)
  },
})
```

## Memory management

Process results incrementally to avoid large aggregations:

```ts
async function* processLargeDataset(items: string[]) {
  const batchSize = 100
  for (let i = 0; i < items.length; i += batchSize) {
    const batch = items.slice(i, i + batchSize)
    const handles = await Promise.all(batch.map((item) => executor.enqueueTask(processTask, { item })))
    for (const handle of handles) {
      yield await handle.waitAndGetFinishedExecution()
    }
  }
}
```

Parent tasks can orchestrate smaller batches:

```ts
const efficientParentTask = executor.parentTask({
  id: 'batchProcessor',
  timeoutMs: 60_000,
  runParent: (ctx, input: { items: string[] }) => {
    const batchSize = 50
    const children = [] as Array<{ task: typeof batchTask; input: { batch: string[] } }>
    for (let i = 0; i < input.items.length; i += batchSize) {
      children.push({ task: batchTask, input: { batch: input.items.slice(i, i + batchSize) } })
    }
    return { output: { totalBatches: children.length }, children }
  },
})
```

## Monitoring

Track:

- Task duration percentiles (p50, p95, p99)
- Failure rates by error type
- Storage operation latency
- Queue depth by status
- Memory/CPU usage

## Load testing

```ts
it('handles 1000 concurrent tasks', async () => {
  const taskCount = 1000
  const handles = await Promise.all(
    Array.from({ length: taskCount }, (_, i) => executor.enqueueTask(quickTask, { id: i })),
  )
  const results = await Promise.all(handles.map((h) => h.waitAndGetFinishedExecution()))
  expect(results.filter((r) => r.status === 'completed')).toHaveLength(taskCount)
})
```

## CPU optimization

```ts
import { Worker, isMainThread, parentPort, workerData } from 'worker_threads'

const cpuIntensiveTask = executor.task({
  id: 'heavyComputation',
  timeoutMs: 300_000,
  run: async (ctx, input: { data: number[] }) => {
    if (isMainThread) {
      return await new Promise((resolve, reject) => {
        const worker = new Worker(__filename, { workerData: input.data })
        worker.on('message', resolve)
        worker.on('error', reject)
      })
    } else {
      const result = heavyComputation(workerData)
      parentPort?.postMessage(result)
    }
  },
})
```
